# Databricks Feature Preview Status Tracker

> **Disclaimer:** This repository is an independent, automated aggregation of Databricks preview feature information. Feature data is collected from two sources: the Databricks workspace/account preview pages as seen in the browser and the Azure Databricks documentation on Microsoft Learn. All descriptions are copied verbatim from their respective sources. However, the feature names — particularly those sourced from the documentation — are derived from navigation titles and page headings, which may not exactly match the official feature name as used by Databricks. This overview is provided as-is for informational purposes only; no rights can be derived from it. Always consult the [official Databricks documentation](https://learn.microsoft.com/en-gb/azure/databricks/) and your workspace's Previews page as the authoritative source.

Tracks Databricks preview features across workspace and account scopes, combining automated API scraping with documentation analysis and LLM-powered classification.

Made by [Cauchy](https://cauchy.io), a Databricks Partner.

> **Prefer the [live site](https://github.com/CauchyIO/dbr_feature_status_tracker/deployments/github-pages)** for a better reading experience — it includes sortable tables, search, and community discussions on individual features.


## AI and machine learning

| Feature | Description | Status (Azure) | Risk Assessment (Human) | Risk Assessment (Claude Opus 4.6) | Source | Last Checked |
|---------|-------------|--------|------------------------|-----------------------------------|--------|--------------|
| [Agent Framework: On-Behalf-Of-User Authorization](https://docs.databricks.com/aws/en/generative-ai/agent-framework/author-agent#on-behalf-of-user-authentication) | This feature enables on-behalf-of-user authentication for generative AI agents deployed via Mosaic AI Agent Framework. When you deploy an agent that performs on-behalf of end user access using Mosaic AI agent framework, the agent will be able to access Databricks resources using the identity of the agent invoker. | PUBLIC_PREVIEW | - | High — agent acts with invoker's identity and permissions | Workspace | 2026-02-05 |
| [AI Classify](https://learn.microsoft.com/en-us/azure/databricks/sql/language-manual/functions/ai_classify) | The ai_classify() function enables you to classify input text directly in SQL using state-of-the-art generative AI models provided by Databricks Foundation Model APIs. By supplying a set of labels, you can declaratively assign categories to unstructured text and iterate on this agent using Databricks Agent Bricks. | PUBLIC_PREVIEW | - | Low | Workspace | 2026-02-05 |
| [AI Extract](https://learn.microsoft.com/en-us/azure/databricks/sql/language-manual/functions/ai_extract) | The ai_extract() function enables you to extract structured entities from unstructured text directly in SQL using state-of-the-art generative AI models provided by Databricks Foundation Model APIs. After specifying a target schema, you can declaratively transform raw text into structured outputs and iterate on this schema and agent using Databricks Agent Bricks. | PUBLIC_PREVIEW | - | Low | Workspace | 2026-02-05 |
| [AI functions](https://learn.microsoft.com/en-gb/azure/databricks/large-language-models/ai-functions) | This article describes Azure Databricks AI Functions and the supported functions. | PUBLIC_PREVIEW | - | Medium — data flows to LLM endpoints including external providers | Docs | 2026-02-11 |
| [AI Gateway](https://learn.microsoft.com/en-gb/azure/databricks/ai-gateway/) | This article describes Mosaic AI Gateway, the Databricks solution for governing and monitoring access to supported generative AI models and their associated model serving endpoints. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-11 |
| [AI Guardrails](https://learn.microsoft.com/en-gb/azure/databricks/ai-gateway/) | AI Guardrails for Mosaic AI Gateway, enabling safety and content filtering on model serving endpoints. Sub-feature of AI Gateway. | PUBLIC_PREVIEW | - | Low — security-enhancing, blocks unsafe content and detects PII | Docs | 2026-02-09 |
| [AI Query for Custom Models and External Models](https://docs.databricks.com/en/large-language-models/how-to-ai-query.html) | Allows ai_query() to connect with Custom Model and External Model endpoints (querying Foundation Model APIs is enabled by default). When enabled, ai_query() used with external models could result in data egress to external model providers. See the documentation for region availability and DBSQL requirements for this functionality. | PUBLIC_PREVIEW | - | High — enables data egress to external model providers | Workspace | 2026-02-05 |
| [Build gen AI apps](https://learn.microsoft.com/en-gb/azure/databricks/generative-ai/agent-framework/teams-agent) | This page shows how to integrate AI agents with Microsoft Teams using OAuth "On Behalf Of" authentication. This integration lets your Databricks agents interact with users through Microsoft Teams while maintaining secure, user-delegated access to Databricks resources. | PUBLIC_PREVIEW | - | High — external AI tools gain direct access to Databricks data | Docs | 2026-02-11 |
| [Build gen AI apps](https://learn.microsoft.com/en-gb/azure/databricks/generative-ai/mcp/connect-external-services) | Connect non-Databricks (external) clients, AI assistants, and IDEs that support Model Context Protocol (MCP) to Databricks MCP servers. This provides access to Databricks data and tools directly in your development environment. | PUBLIC_PREVIEW | - | High — external AI tools gain direct access to Databricks data | Docs | 2026-02-08 |
| [Deploy models](https://learn.microsoft.com/en-gb/azure/databricks/large-language-models/batch-inference-pipelines) | This page shows how you can integrate AI Functions into other Databricks data and AI products to build complete batch inference pipelines. These pipelines can perform end-to-end workflows that include ingestion, preprocessing, inference, and post-processing. Pipelines can be authored in SQL or Python and deployed as: | PUBLIC_PREVIEW | - | Medium — amplifies data egress and cost at scale via LLM endpoints | Docs | 2026-02-11 |
| [Deploy models](https://learn.microsoft.com/en-gb/azure/databricks/machine-learning/model-serving/enable-model-serving-inference-tables) | This article describes the legacy inference table experience, which is only relevant for certain provisioned throughput and custom model serving endpoints. | PUBLIC_PREVIEW | - | - | Docs | 2026-02-11 |
| [External Tool Calling for Agents](https://docs.databricks.com/en/generative-ai/agent-framework/external-connection-tools.html) | Create AI agent tools that can connect to external applications with an API using HTTP requests | PUBLIC_PREVIEW | - | High — agent tools make HTTP calls to external APIs | Workspace | 2026-02-05 |
| [Feature management](https://learn.microsoft.com/en-gb/azure/databricks/machine-learning/feature-store/migrate-from-online-tables) | This page describes how to migrate your existing online tables. You can migrate to the following: | PUBLIC_PREVIEW | - | Low | Workspace | 2026-02-08 |
| [Integrations](https://learn.microsoft.com/en-gb/azure/databricks/large-language-models/) | Azure Databricks makes it simple to access and build off of publicly available large language models. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-11 |
| Managed MCP Servers | Databricks managed MCP (Model Context Protocol) servers enable your AI agents to access data and tools governed in Databricks, with data governance and permissions enforced out of the box | PUBLIC_PREVIEW | - | Medium — MCP servers access governed data on agent's behalf | Workspace | 2026-02-05 |
| [MLflow for models](https://learn.microsoft.com/en-gb/azure/databricks/mlflow/build-dashboards) | Using MLflow metadata in system tables, you can build dashboards to analyze your MLflow experiments and runs from the entire workspace. Using the existing MLflow UI and REST APIs for these tasks would require extensive, time-consuming iteration. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-08 |
| [Models in Unity Catalog: Deployment Jobs](https://learn.microsoft.com/en-us/azure/databricks/mlflow/deployment-job) | Deployment jobs allow you to manage the model lifecycle by automating tasks like evaluation, approval, and deployment whenever a new model version is created, integrating seamlessly with Unity Catalog models and Databricks Jobs. These jobs simplify the setup of model deployment pipelines, incorporate human-in-the-loop approvals, and provide governed workflows with clear visibility into progress and historical context for each model version. | PUBLIC_PREVIEW | - | Low | Workspace | 2026-02-05 |
| [Serve data for AI](https://learn.microsoft.com/en-gb/azure/databricks/vector-search/query-vector-search) | This article describes how to query a vector search index, including how to use filters and reranking. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-11 |
| [Storage Optimized Vector Search](https://docs.databricks.com/en/generative-ai/vector-search#endpoint-options) | This preview introduces Storage Optimized Vector Search. Storage Optimized Vector Search is engineered from the ground up to support massive-scale use cases, with the capacity to handle tens of billions of embeddings. It delivers a dramatically lower price-per-vector cost and significantly enhances ingestion performance, offering a powerful alternative to the Standard Vector Search product for demanding, large-scale applications. | PUBLIC_PREVIEW | - | Low | Workspace | 2026-02-05 |
| [Synthetic Agent Evaluation](https://docs.databricks.com/en/generative-ai/agent-evaluation/synthesize-evaluation-set.html) | Synthetically generate a high-quality evaluation set for measuring the quality of your agent. | PUBLIC_PREVIEW | - | Low | Workspace | 2026-02-05 |
| [Train models](https://learn.microsoft.com/en-gb/azure/databricks/large-language-models/foundation-model-training/) | This feature is in Public Preview in the following regions: centralus, eastus, eastus2, northcentralus, and westus. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-11 |
| [Train models](https://learn.microsoft.com/en-gb/azure/databricks/machine-learning/train-model/sparkdl-xgboost) | sparkdl.xgboost is deprecated starting with Databricks Runtime 12.0 ML, and is removed in Databricks Runtime 13.0 ML and above. For information about migrating your workloads to xgboost.spark, see Migration guide for the deprecated sparkdl.xgboost module. | PUBLIC_PREVIEW | - | - | Docs | 2026-02-11 |
| [Train models](https://learn.microsoft.com/en-gb/azure/databricks/machine-learning/train-model/serverless-forecasting) | This article shows you how to run a serverless forecasting experiment using the Mosaic AI Model Training UI. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-11 |
| [Tutorials](https://learn.microsoft.com/en-gb/azure/databricks/large-language-models/foundation-model-training/fine-tune-run-tutorial) | This feature is in Public Preview in the following regions: centralus, eastus, eastus2, northcentralus, and westus. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-11 |
| Dashboard Authoring Agent | Enables the Dashboard Authoring Agent, which lets users create and edit AI/BI Dashboards using the Databricks Assistant Agent Mode | BETA | - | Medium — partner AI processes data, autonomous table exploration | Workspace | 2026-02-11 |
| [Enable Qwen Model Family Endpoints](https://docs.databricks.com/aws/en/machine-learning/foundation-model-apis/supported-models#qwen3-next-instruct) | This preview enables all Qwen endpoints on model serving surfaces. | BETA | - | Low | Workspace | 2026-02-05 |
| [Managed MLflow Prompt Registry](https://learn.microsoft.com/en-us/azure/databricks/mlflow3/genai/prompt-version-mgmt/prompt-registry/create-and-edit-prompts) | Managed MLflow Prompt Registry on Databricks is a powerful tool that streamlines prompt engineering and management in your Generative AI (GenAI) applications. It enables you to version, track, and reuse prompts across your organization, helping maintain consistency and improving collaboration in prompt development.  The Prompt Registry also enables you to optimize prompts through a native integration with DSPy, delivering higher quality while minimizing time spent on manual prompt engineering. | BETA | - | Low | Workspace | 2026-02-05 |
| [MLflow for gen AI apps](https://learn.microsoft.com/en-gb/azure/databricks/mlflow3/genai/api-reference) | This page provides an index of important MLflow APIs used in GenAI applications, with direct links to the official MLflow documentation. | BETA | - | Low | Docs | 2026-02-11 |
| [Mosaic AI Agent Bricks Preview](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/ai-builder/) | Mosaic AI Agent Bricks enables you to build and optimize domain-specific agent systems. Simply select your problem, and Agent Bricks will run optimizations to improve quality and cost, generating a deployable endpoint on Databricks. Additionally, Agent Bricks provides automated evaluation and recommendations to refine your agent systems performance. Enabling this flag will also enable ai_parse_document function; however, for the regions where Agent Bricks is not available, you will only get access to ai_parse_document without Agent Bricks enabled. | BETA | - | Medium — AI agent bricks deploy endpoints, review access controls | Workspace | 2026-02-05 |
| [OpenTelemetry on Databricks](https://learn.microsoft.com/en-us/azure/databricks/mlflow3/genai/tracing/trace-unity-catalog) | Enables ingestion of OpenTelemetry data into Unity Catalog managed Delta tables for MLflow Tracing. For direct ingestion via OTLP endpoints, see https://docs.google.com/document/d/1_cMNMbepAy2rt0QuVlSKgWR7bLSvXmVar-BSIdvvAKY. | BETA | - | Low | Workspace | 2026-02-05 |
| [Production Monitoring for MLflow](https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/production-monitoring) | This beta enables monitoring of any Generative AI app or agent deployed outside of Databricks OR on Databricks using MLflow 3.0. It provides developers with tools to track both performance metrics (latency, request volume, errors) and quality metrics (accuracy, correctness, compliance), allowing them to detect drift or regressions via LLM-based evaluations on production traffic. Developers can also deep dive into individual requests for debugging and improvement purposes, and export real-world logs into evaluation sets to drive continuous enhancements of their Generative AI applications. | BETA | - | Low | Workspace | 2026-02-05 |
| [Train models](https://learn.microsoft.com/en-gb/azure/databricks/compute/serverless/gpu) | This feature is in Beta. Workspace admins can control access to this feature from the Previews page. See Manage Azure Databricks previews. | BETA | - | Medium — no Private Link support, excluded from compliance profiles (HIPAA/PCI) | Docs | 2026-02-08 |
| [Vector Search Full Text](https://docs.databricks.com/aws/en/generative-ai/create-query-vector-search#query-a-vector-search-endpoint) | When enabled, users can run full-text search queries directly on indices. This bypasses ANN entirely and is especially valuable in scenarios where precise keyword matching is required. | BETA | - | Low | Workspace | 2026-02-05 |

## Administration

| Feature | Description | Status (Azure) | Risk Assessment (Human) | Risk Assessment (Claude Opus 4.6) | Source | Last Checked |
|---------|-------------|--------|------------------------|-----------------------------------|--------|--------------|
| [Audit logs: access request destination events](https://learn.microsoft.com/en-gb/azure/databricks/admin/account-settings/audit-logs) | Audit log events for access request destinations (accessRequestDestination service). Sub-feature of Diagnostic logs. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-09 |
| [Audit logs: account access control events](https://learn.microsoft.com/en-gb/azure/databricks/admin/account-settings/audit-logs) | Audit log events for account-level access control operations (accountsAccessControl service). Sub-feature of Diagnostic logs. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-09 |
| [Audit logs: Delta Sharing Iceberg client events](https://learn.microsoft.com/en-gb/azure/databricks/admin/account-settings/audit-logs) | Audit log events for Delta Sharing Iceberg client access (deltaSharingIcebergClient service). Sub-feature of Diagnostic logs. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-09 |
| [Audit logs: service principal credentials events](https://learn.microsoft.com/en-gb/azure/databricks/admin/account-settings/audit-logs) | Audit log events for service principal credential management (servicePrincipalCredentials service). Sub-feature of Diagnostic logs. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-09 |
| [Budget Policy](https://learn.microsoft.com/en-us/azure/databricks/admin/usage/budget-policies) | Allow billing admins to enforce tagging requirements across serverless workloads such as workflows, notebooks, DLT pipelines, model-serving, and Databrick Apps. | PUBLIC_PREVIEW | - | Low | Account | 2026-02-05 |
| [Immutable external groups](https://learn.microsoft.com/en-us/azure/databricks/admin/users-groups/groups#account-vs-workspace-group) | External groups are groups that are created using a SCIM provisioning connector. When enabled, external groups can not be updated using the Databricks UI. | PUBLIC_PREVIEW | - | Low | Account | 2026-02-05 |
| [MLflow Metadata System Table](https://learn.microsoft.com/en-us/azure/databricks/admin/system-tables/mlflow) | Enables the system.mlflow schema, which contains metadata recorded for MLflow experiments and runs. This preview no longer requires enrollment as the feature is now in public preview with the schema available to customers by default, please see the documentation for how account admins can share schema access with users. | PUBLIC_PREVIEW | - | Low | Account | 2026-02-05 |
| [Monitor account activity with system tables](https://learn.microsoft.com/en-gb/azure/databricks/admin/system-tables/audit-logs) | This article outlines the audit log table schema and has sample queries you can use with the audit log system table to answer common account activity questions. For information on audit log events, see Diagnostic log reference. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-08 |
| [Monitor account activity with system tables](https://learn.microsoft.com/en-gb/azure/databricks/admin/system-tables/clean-rooms) | The clean room events table records actions taken by you or your collaborators on clean rooms in your account. This table includes regional data from across your account. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-08 |
| [Monitor account activity with system tables](https://learn.microsoft.com/en-gb/azure/databricks/admin/system-tables/data-classification) | This page outlines the data classification results table schema and includes sample queries. The table stores detections for sensitive data classes at the column level across enabled catalogs in your metastore. | PUBLIC_PREVIEW | - | Medium — contains sample PII values and maps sensitive data locations | Docs | 2026-02-11 |
| [Monitor account activity with system tables](https://learn.microsoft.com/en-gb/azure/databricks/admin/system-tables/data-quality-monitoring) | This page outlines the data quality monitoring results system table schema and includes sample queries. The table stores results of freshness and completeness checks, as well as downstream impact and root cause analysis, across all tables enabled for data quality monitoring in your metastore. | PUBLIC_PREVIEW | - | Medium — contains sample values, reveals data dependency graph | Docs | 2026-02-11 |
| [Monitor account activity with system tables](https://learn.microsoft.com/en-gb/azure/databricks/admin/system-tables/assistant) | This page includes a schema reference of the Databricks Assistant events system table and an example that uses the data in a dashboard. Each row in this system table represents a message sent by a user in either the Assistant window or the in-cell Assistant. For more information about the Databricks Assistant, see What is Databricks Assistant?. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-11 |
| [Monitor account activity with system tables](https://learn.microsoft.com/en-gb/azure/databricks/admin/system-tables/marketplace) | This page provides a reference on how to use system tables to help monitor your Databricks Marketplace selling process. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-11 |
| [Monitor account activity with system tables](https://learn.microsoft.com/en-gb/azure/databricks/admin/system-tables/network) | The network access events tables record events where network access is denied. Each row represents an individual event, such as a blocked outbound request to an external domain or a blocked inbound request from a restricted IP. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-08 |
| [Monitor account activity with system tables](https://learn.microsoft.com/en-gb/azure/databricks/admin/system-tables/predictive-optimization) | To have access to this table, your region must support predictive optimization. See Azure Databricks regions. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-08 |
| [Monitor account activity with system tables](https://learn.microsoft.com/en-gb/azure/databricks/admin/system-tables/query-history) | This article includes information on the query history system table, including an outline of the table's schema. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-11 |
| [Monitor account activity with system tables](https://learn.microsoft.com/en-gb/azure/databricks/admin/system-tables/warehouse-events) | In this article, you learn how to use the warehouse events system table to monitor and manage the SQL warehouses in your workspaces. This table records a row for every time a warehouse starts, stops, runs, and scales up and down. You can use the sample queries in this article with alerts to keep you informed of changes to your warehouses. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-11 |
| [Monitor account activity with system tables](https://learn.microsoft.com/en-gb/azure/databricks/admin/system-tables/warehouses) | In this article, you learn how to use the warehouses system table to monitor and manage the SQL warehouses in your workspaces. Each row is a snapshot of the SQL warehouse properties at that moment. A new snapshot is created when the properties change. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-11 |
| [Monitor account activity with system tables](https://learn.microsoft.com/en-gb/azure/databricks/admin/system-tables/) | This article explains the concept of system tables in Azure Databricks and highlights resources you can use to get the most out of your system tables data. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-11 |
| [Monitor account activity with system tables](https://learn.microsoft.com/en-gb/azure/databricks/admin/system-tables/workspaces) | This page explains how to use the workspaces system table to monitor workspaces in your Azure Databricks account. Each row in the table represents the latest known state of an active workspace in your account, including metadata and lifecycle status. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-11 |
| [Sharing Materialization History System Table](https://docs.databricks.com/aws/en/admin/system-tables/materialization) | Enables the system.sharing schema, which contains one system table: materialization_history. The schema will become visible in your metastore within 24h. Note that you must grant SELECT rights to the users as described in the documentation. | PUBLIC_PREVIEW | - | Low | Account | 2026-02-05 |
| [Workspace base environments for serverless compute](https://learn.microsoft.com/en-us/azure/databricks/admin/workspace-settings/base-environment) | Workspace base environments ensure that users automatically have a predefined environment version and set of Python packages available in their notebooks. | PUBLIC_PREVIEW | - | Low | Workspace | 2026-02-05 |
| [Audit logs: SQL Alerts events](https://learn.microsoft.com/en-gb/azure/databricks/admin/account-settings/audit-logs) | Audit log events for SQL Alerts (alertsV2 service). Sub-feature of Diagnostic logs. | BETA | - | Low | Docs | 2026-02-09 |
| [Diagnostic logs](https://learn.microsoft.com/en-gb/azure/databricks/admin/account-settings/audit-logs) | This article provides you with a comprehensive reference of audit log services and events. The availability of these services depends on how you access the logs: | BETA | - | Low | Docs | 2026-02-11 |
| [Monitor account activity with system tables](https://learn.microsoft.com/en-gb/azure/databricks/admin/system-tables/zerobus-ingest) | This article is a reference for the zerobus system tables, which track Zerobus Ingest activity in your workspace. These tables include your account records from all workspaces in your same region. To see records from another region, you must view the tables from a workspace deployed in that region. | BETA | - | Low | Docs | 2026-02-11 |
| [Monitor account activity with system tables](https://learn.microsoft.com/en-gb/azure/databricks/admin/system-tables/jobs) | The lakeflow schema was previously known as workflow. The content of both schemas is identical. | GA | - | Low | Docs | 2026-02-08 |
| [Serverless Workspaces](https://learn.microsoft.com/en-us/azure/databricks/admin/workspace/serverless-workspaces) | Serverless workspaces are Databricks-managed workspaces that come pre-configured with Unity Catalog, serverless compute and default storage, providing a lightweight, fast-to-deploy full SaaS workspace experience. | GA | - | Medium — serverless workspaces shift control to Databricks-managed infra | Account | 2026-02-05 |

## Apps

| Feature | Description | Status (Azure) | Risk Assessment (Human) | Risk Assessment (Claude Opus 4.6) | Source | Last Checked |
|---------|-------------|--------|------------------------|-----------------------------------|--------|--------------|
| [Configure apps](https://learn.microsoft.com/en-gb/azure/databricks/dev-tools/databricks-apps/tags) | Use tags to organize and categorize Databricks apps for easier management. Databricks Apps also support certification and deprecation system tags to indicate trust or lifecycle status. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-08 |
| [Databricks Apps - On-Behalf-Of User Authorization](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/databricks-apps/app-development#-using-the-databricks-apps-authorization-model) | Allows the Databricks App to act on behalf of the app user. This enhancement allows the app to honor the user's access permissions defined in Unity Catalog and in Databricks Workspace. | PUBLIC_PREVIEW | - | High — apps act on behalf of user, broad permission scope | Workspace | 2026-02-05 |
| [Databricks Apps - Install Apps from Git](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/databricks-apps/deploy/#deploy-from-a-git-repository) | Deploy apps directly from Git into the Apps Runtime for better governance and production-quality control. | BETA | - | - | Workspace | 2026-02-11 |
| [Databricks Apps – Configure App Compute Size](https://learn.microsoft.com/en-us/azure/databricks/dev-tools/databricks-apps/compute-size) | Allows app developers to select predefined compute sizes for Databricks Apps. This feature enables building apps that require more CPU or memory to handle larger workloads. | GA | - | Low | Workspace | 2026-02-05 |

## Business intelligence

| Feature | Description | Status (Azure) | Risk Assessment (Human) | Risk Assessment (Claude Opus 4.6) | Source | Last Checked |
|---------|-------------|--------|------------------------|-----------------------------------|--------|--------------|
| [Admin guide](https://learn.microsoft.com/en-gb/azure/databricks/ai-bi/admin/audit) | This article has sample queries that workspace admins can use to monitor activity associated with dashboards and Genie spaces. All queries access the audit logs table, which is a system table that stores records for all audit events from workspaces in your region. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-11 |
| [AI Forecast](https://learn.microsoft.com/en-gb/azure/databricks/dashboards/) | You can use dashboards to build data visualizations and share reports with your team. AI/BI dashboards feature AI-assisted authoring, an enhanced visualization library, and a streamlined configuration experience so that you can quickly transform data into sharable insights. When published, your dashboards can be shared with anyone registered to your Azure Databricks account, even if they don't have access to the workspace. See Share a dashboard. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-11 |
| [Custom calculations > Metric view integration](https://learn.microsoft.com/en-gb/azure/databricks/dashboards/custom-calculations/) | Define custom calculations on top of a dataset created by a metric view. Sub-feature of Custom calculations. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-09 |
| [Dashboard embedding for external users](https://learn.microsoft.com/en-gb/azure/databricks/dashboards/embedding/) | Embed published dashboards for viewers outside your Databricks workspace. Sub-feature of dashboard embedding. | PUBLIC_PREVIEW | - | Medium — external users view data without Databricks accounts via service principal | Docs | 2026-02-09 |
| [Dashboard tags](https://learn.microsoft.com/en-gb/azure/databricks/dashboards/) | You can use dashboards to build data visualizations and share reports with your team. AI/BI dashboards feature AI-assisted authoring, an enhanced visualization library, and a streamlined configuration experience so that you can quickly transform data into sharable insights. When published, your dashboards can be shared with anyone registered to your Azure Databricks account, even if they don't have access to the workspace. See Share a dashboard. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-11 |
| [Dashboards](https://learn.microsoft.com/en-gb/azure/databricks/dashboards/) | You can use dashboards to build data visualizations and share reports with your team. AI/BI dashboards feature AI-assisted authoring, an enhanced visualization library, and a streamlined configuration experience so that you can quickly transform data into sharable insights. When published, your dashboards can be shared with anyone registered to your Azure Databricks account, even if they don't have access to the workspace. See Share a dashboard. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-11 |
| [Dashboards with Genie spaces](https://learn.microsoft.com/en-gb/azure/databricks/dashboards/genie-spaces) | Published dashboards include a Genie space by default to allow business users to explore data using natural language. A Genie space is a no-code interface that provides an Ask Genie button on published dashboards, allowing viewers to chat with the da... | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-08 |
| [Enable a Genie space from your dashboard](https://learn.microsoft.com/en-gb/azure/databricks/dashboards/) | You can use dashboards to build data visualizations and share reports with your team. AI/BI dashboards feature AI-assisted authoring, an enhanced visualization library, and a streamlined configuration experience so that you can quickly transform data into sharable insights. When published, your dashboards can be shared with anyone registered to your Azure Databricks account, even if they don't have access to the workspace. See Share a dashboard. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-11 |
| [Genie - Upload File](https://docs.databricks.com/en/genie/file-upload) | AI/BI Genie now supports a File Upload functionality in this preview. This feature enables users to enhance their analytics capabilities by blending data from the Unity Catalog with their own personal files. Users can interact with their combined datasets using natural language queries, receiving insightful answers quickly and intuitively. Supported file formats for this preview are CSV and Excel files. To get started, turn this preview on and drag files into any Genie space conversation.  | PUBLIC_PREVIEW | - | Low | Workspace | 2026-02-11 |
| [Manage datasets > Export as metric view](https://learn.microsoft.com/en-gb/azure/databricks/dashboards/datasets) | Export a dashboard dataset as a metric view for centralized business logic. Sub-feature of Manage datasets. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-09 |
| [Manage datasets > Metric view as data source](https://learn.microsoft.com/en-gb/azure/databricks/dashboards/datasets) | Use a metric view as a data source for dashboard datasets. Sub-feature of Manage datasets. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-09 |
| [Power BI task type](https://learn.microsoft.com/en-us/azure/databricks/jobs/powerbi) | The Power BI task type in Databricks Workflows allows users to keep Power BI semantic models up-to-date with source data in Unity Catalog. The task can sync table metadata, evolving schemas, and primary/foreign key relationships. It works for DirectQuery and Import mode at the table level, allowing you to define composite models. | PUBLIC_PREVIEW | - | Low | Workspace | 2026-02-05 |
| [Support Dashboards in Git Folder](https://docs.databricks.com/en/dashboards/git-support.html) | Enable version control for Dashboards within Git Folder | PUBLIC_PREVIEW | - | Low | Workspace | 2026-02-05 |
| [Genie Answer Inspection](https://learn.microsoft.com/en-us/azure/databricks/genie/#inspect-mode) | Advanced technique that reviews initial SQL answers and makes improvements in standard Genie | BETA | - | - | Workspace | 2026-02-11 |
| [Genie Research Agent](https://learn.microsoft.com/en-us/azure/databricks/genie/research-agent) | The Genie Research Agent provides deeper data insights and answers complex business questions using multi-step reasoning and hypothesis investigation. | BETA | - | Medium — multi-step AI agent reasons over your data | Workspace | 2026-02-05 |
| [Custom calculations](https://learn.microsoft.com/en-gb/azure/databricks/dashboards/custom-calculations/) | Custom calculations let you define dynamic metrics and transformations without modifying dataset queries. This page explains how to use custom calculations in AI/BI dashboards. | GA | - | Low | Docs | 2026-02-08 |
| [Dashboard settings](https://learn.microsoft.com/en-gb/azure/databricks/dashboards/settings) | This page explains how to customize dashboard settings and themes to match your organization's branding, improve readability, and ensure consistent data presentation across different regions and languages. | GA | - | Low | Docs | 2026-02-08 |
| [Embed a dashboard](https://learn.microsoft.com/en-gb/azure/databricks/dashboards/embedding/) | This page shows how to embed an AI/BI dashboard in an external website or application. Work with published dashboards Only published dashboards can be embedded into external applications. | GA | - | Low | Docs | 2026-02-08 |
| [Manage datasets](https://learn.microsoft.com/en-gb/azure/databricks/dashboards/datasets) | This article explains how to create and manage dashboard datasets using the dataset editor in an AI/BI dashboard. Define datasets To define or access existing datasets, click the Data tab near the upper-left corner of your dashboard. | GA | - | Low | Docs | 2026-02-08 |

## Compute

| Feature | Description | Status (Azure) | Risk Assessment (Human) | Risk Assessment (Claude Opus 4.6) | Source | Last Checked |
|---------|-------------|--------|------------------------|-----------------------------------|--------|--------------|
| [High memory option for serverless notebooks & jobs](https://learn.microsoft.com/en-us/azure/databricks/compute/serverless/dependencies#high-memory) | This preview brings the ability to increase available memory for serverless notebooks and jobs to help customers avoid out-of-memory crashes. | PUBLIC_PREVIEW | - | Low | Workspace | 2026-02-05 |
| [Warehouse Activity Details](https://learn.microsoft.com/azure/databricks/compute/sql-warehouse/monitor#metrics) | Provides deeper visibility into SQL Warehouse usage by showing why a warehouse is running even when no queries are visible. The SQL Warehouse monitoring UI includes an activity details view in the running clusters chart, showing query execution and client-driven activity, such as open sessions or query fetching. | BETA | - | Low | Workspace | 2026-02-05 |
| [Classic compute](https://learn.microsoft.com/en-gb/azure/databricks/compute/dedicated-overview) | This page provides an overview of dedicated compute access mode. | GA | - | Low | Docs | 2026-02-11 |
| [Classic compute](https://learn.microsoft.com/en-gb/azure/databricks/libraries/notebooks-python-libraries) | Notebook-scoped libraries let you create, modify, save, reuse, and share custom Python environments that are specific to a notebook. When you install a notebook-scoped library, only the current notebook and any jobs associated with that notebook have access to that library. Other notebooks attached to the same cluster are not affected. | GA | - | Low | Docs | 2026-02-11 |
| [Cluster Log Delivery to UC Volumes](https://learn.microsoft.com/en-us/azure/databricks/compute/configure#cluster-log-delivery) | Allows users to configure a classic compute cluster’s cluster log delivery to a UC volume path through UI or API. Cluster logs for the Spark driver node, worker nodes and events will be delivered to the configured volume path. | GA | - | Low | Workspace | 2026-02-11 |
| [Default Python package repositories in clusters created via API](https://learn.microsoft.com/en-us/azure/databricks/admin/workspace-settings/default-python-packages) | Apply the default Python package repositories in clusters created via API. The changes of configurations apply to newly created clusters and existing clusters upon restart. | GA | - | Medium — alters package resolution on API clusters | Workspace | 2026-02-11 |
| [Default Python package repositories in clusters created via UI](https://learn.microsoft.com/en-us/azure/databricks/admin/workspace-settings/default-python-packages) | Apply the default Python package repositories in clusters created via UI. The changes of configurations apply to newly created clusters and existing clusters upon restart. | GA | - | Medium — alters package resolution on UI clusters | Workspace | 2026-02-11 |

## Data engineering

| Feature | Description | Status (Azure) | Risk Assessment (Human) | Risk Assessment (Claude Opus 4.6) | Source | Last Checked |
|---------|-------------|--------|------------------------|-----------------------------------|--------|--------------|
| [@materialized_view row_filter parameter](https://learn.microsoft.com/en-gb/azure/databricks/ldp/developer/ldp-python-ref-materialized-view) | The row_filter parameter for @materialized_view to apply row-level filters in pipelines. Sub-feature of @materialized_view. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-09 |
| [Enables remote query table-valued function (remote_query).](https://docs.databricks.com/aws/en/query-federation/remote-queries) | Function allows users to execute query in remote engine syntax using credentials from a Unity Catalog connection. Function is available on Databricks Runtime 17.3 or above. | PUBLIC_PREVIEW | - | Medium — executes queries on remote engines via credentials | Workspace | 2026-02-05 |
| [Join Pushdown for Federated Queries](https://docs.databricks.com/aws/en/query-federation/performance-recommendations#join-pushdown-in-lakehouse-federation) | Enables automatic pushdown of join operations to remote databases when executing federated queries through Lakehouse Federation. When enabled, Databricks pushes inner, left, and right joins between tables from the same JDBC datasource (Oracle, PostgreSQL, MySQL, SQL Server, Teradata) directly to the remote database engine, reducing data transfer and improving query performance. This feature is available on Databricks Runtime 17.2 and above. | PUBLIC_PREVIEW | - | Low | Workspace | 2026-02-05 |
| [Lakeflow Connect](https://learn.microsoft.com/en-gb/azure/databricks/ingestion/data-migration/clone-parquet) | You can use Azure Databricks clone functionality to incrementally convert data from Parquet or Apache Iceberg data sources to managed or external Delta tables. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-08 |
| [Lakeflow Connect Column Selection for Database Sources](https://learn.microsoft.com/en-us/azure/databricks/ingestion/lakeflow-connect/column-selection) | Enable column selection for Lakeflow Connects SQL Server connector. Available only via API. | PUBLIC_PREVIEW | - | Low | Workspace | 2026-02-05 |
| [Lakeflow Connect for Dynamics 365](https://learn.microsoft.com/en-us/azure/databricks/ingestion/lakeflow-connect/d365-source-setup) | Ingest Dynamics 365 data via Dataverse with a simple and efficient connector. Available via API or UI. | PUBLIC_PREVIEW | - | Low | Workspace | 2026-02-05 |
| [Lakeflow Connect for Netsuite](https://learn.microsoft.com/en-us/azure/databricks/ingestion/lakeflow-connect/netsuite-source-setup) | Ingest NetSuite data with a simple and efficient connector. Available via API only. | PUBLIC_PREVIEW | - | Low | Workspace | 2026-02-05 |
| [Lakeflow Pipelines Editor](https://learn.microsoft.com/en-us/azure/databricks/dlt/dlt-multi-file-editor) | Purpose-built IDE for declarative data pipelines. Designed to support everything you need for building pipelines in one place: code-first authoring, folder-based organization, selective execution, data previews, and pipeline graphs. Integrated with the Databricks Platform, supporting version control, code reviews, and scheduling. | PUBLIC_PREVIEW | - | Low | Workspace | 2026-02-05 |
| [Lakeflow Spark Declarative Pipelines](https://learn.microsoft.com/en-gb/azure/databricks/ldp/monitoring-ui) | This section describes using built-in monitoring and observability features for Lakeflow Spark Declarative Pipelines in the Azure Databricks user interface. These features support tasks such as: | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-11 |
| [Lakeflow Spark Declarative Pipelines](https://learn.microsoft.com/en-gb/azure/databricks/ldp/dbsql/materialized-monitor) | This article describes how to monitor and query refresh data about a materialized view in Databricks SQL. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-11 |
| [SFTP Connector](https://learn.microsoft.com/azure/databricks/ingestion/sftp) | Ingest files from SFTP server using Auto Loader. Requires DBR 17.3+. | PUBLIC_PREVIEW | - | Low | Workspace | 2026-02-05 |
| [Unified Runs List](https://docs.google.com/document/d/1ga9tv2ctn6-OtzgkomqbaJe8BTPVsYb_UgZOXEJnJ_M) | You can now view all of your Jobs and Pipeline executions in the updated Runs list. Track all of your pipeline executions in one place and filter by status, time, run-as user, and error codes in real time. Spot trends using the visualization and summary of the current top five error codes. | PUBLIC_PREVIEW | - | Low | Workspace | 2026-02-05 |
| [dbt platform task](https://learn.microsoft.com/en-us/azure/databricks/jobs/dbt-platform) | The dbt platform task lets you run your dbt platform jobs as part of existing databricks workflows | BETA | - | Low | Workspace | 2026-02-05 |
| [Excel File Format Support](https://learn.microsoft.com/en-us/azure/databricks/query/formats/excel) | Read Excel files using Spark batch and streaming APIs including Auto Loader, read_files, spark.read and COPY INTO. Available in DBR17.1+ | BETA | - | Low | Workspace | 2026-02-05 |
| [Lakeflow Connect for Confluence](https://learn.microsoft.com/en-gb/azure/databricks/ingestion/lakeflow-connect/confluence-source-setup) | Ingest from Confluence with a simple and efficient connector | BETA | - | Low | Workspace | 2026-02-05 |
| [Lakeflow Connect for Google Ads](https://learn.microsoft.com/en-in/azure/databricks/ingestion/lakeflow-connect/google-ads-overview) | Ingest from Google Ads with a simple and efficient connector. | BETA | - | - | Workspace | 2026-02-11 |
| [Lakeflow Connect for Google Drive](https://learn.microsoft.com/en-us/azure/databricks/ingestion/google-drive) | Ingest from Google Drive with a simple and efficient connector. **Requires DBR 17.3+** | BETA | - | Low | Workspace | 2026-02-05 |
| [Lakeflow Connect for Jira](https://learn.microsoft.com/en-gb/azure/databricks/ingestion/lakeflow-connect/jira-source-setup) | Ingest Jira data with a simple and efficient connector. Available via API for both Jira Cloud and on premise instances. | BETA | - | Low | Workspace | 2026-02-05 |
| [Lakeflow Connect for Meta Ads](https://learn.microsoft.com/en-gb/azure/databricks/ingestion/lakeflow-connect/meta-ads-source-setup) | Ingest Meta Ads data with a simple and efficient connector. | BETA | - | Low | Workspace | 2026-02-05 |
| [Lakeflow Connect for Sharepoint](https://learn.microsoft.com/en-us/azure/databricks/ingestion/sharepoint) | Ingest Sharepoint data with a simple and efficient connector. Available via API. | BETA | - | Low | Workspace | 2026-02-05 |
| [Lakeflow Connect for Zendesk](https://learn.microsoft.com/en-gb/azure/databricks/connect/managed-ingestion) | Ingest Zendesk Support data with a managed ingestion connector. | BETA | - | Low | Docs | 2026-02-09 |
| [Lakeflow Connect for Zendesk Support](https://learn.microsoft.com/en-us/azure/databricks/ingestion/lakeflow-connect/zendesk-support-overview) | Ingest from Zendesk Support with a simple and efficient connector. | BETA | - | - | Workspace | 2026-02-11 |
| [Concepts](https://learn.microsoft.com/en-gb/azure/databricks/delta/selective-overwrite) | Azure Databricks leverages Delta Lake functionality to support two distinct options for selective overwrites: | GA | - | Low | Docs | 2026-02-08 |
| [Default Python package repositories in Spark Declarative Pipelines](https://learn.microsoft.com/en-us/azure/databricks/admin/workspace-settings/default-python-packages) | Default Python package repositories in serverless and classic Spark Declarative Pipelines (SDP). The changes of configurations apply to new pipeline runs. | GA | - | Medium — alters package resolution in pipelines | Workspace | 2026-02-11 |
| [Lakeflow Jobs](https://learn.microsoft.com/en-gb/azure/databricks/jobs/configure-job) | You can create and run a job using the Jobs UI, or developer tools such as the Databricks CLI or the REST API. Using the UI or API, you can repair and rerun a failed or canceled job. This article shows how to create, configure, and edit jobs using the Jobs & Pipelines workspace UI. For information about other tools, see the following: | GA | - | Low | Docs | 2026-02-11 |
| [Lakeflow Spark Declarative Pipelines](https://learn.microsoft.com/en-gb/azure/databricks/ldp/serverless) | This article describes configurations for serverless pipelines. | GA | - | Low | Docs | 2026-02-11 |
| [Lakeflow Spark Declarative Pipelines](https://learn.microsoft.com/en-gb/azure/databricks/ldp/developer/ldp-python-ref-materialized-view) | The @materialized_view decorator can be used to define materialized views in a pipeline. | GA | - | Low | Docs | 2026-02-11 |

## Data governance

| Feature | Description | Status (Azure) | Risk Assessment (Human) | Risk Assessment (Claude Opus 4.6) | Source | Last Checked |
|---------|-------------|--------|------------------------|-----------------------------------|--------|--------------|
| [Attribute Based Access Control](https://docs.databricks.com/en/data-governance/unity-catalog/abac) | Attribute Based Access Control (ABAC) in Unity Catalog enables scalable, fine-grained data governance by dynamically enforcing access control policies based on tags and user principals. Administrators can define policies once at the catalog, schema, or table level, using tags to selectively apply row filters or column masks without manually configuring individual securables, streamlining governance across the Lakehouse. | PUBLIC_PREVIEW | - | Medium — policy misconfiguration can leak rows/columns | Account | 2026-02-05 |
| [Attribute Based Access Control in Delta Sharing](https://learn.microsoft.com/en-us/azure/databricks/delta-sharing/create-share#abac) | This feature allows privileged users on the provider side to share ABAC-enabled assets. | PUBLIC_PREVIEW | - | Medium — ABAC sharing exposes data to external recipients | Account | 2026-02-05 |
| [Data classification](https://learn.microsoft.com/en-gb/azure/databricks/data-governance/unity-catalog/data-classification) | This page describes how to use Databricks Data Classification in Unity Catalog to automatically classify and tag sensitive data in your catalog. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-11 |
| [Data Classification](https://docs.databricks.com/en/lakehouse-monitoring/data-classification) | This preview allows you to classify and review sensitive data across your entire catalog. This version delivers higher quality detection using a LLM-powered classification engine hosted by Databricks. Customer data will not be retained or shared with Databricks or any third parties. | PUBLIC_PREVIEW | - | Low | Workspace | 2026-02-08 |
| [Flag certified and deprecated data](https://learn.microsoft.com/en-gb/azure/databricks/data-governance/unity-catalog/certify-deprecate-data) | This page shows how to apply system tags to objects to mark them as certified or deprecated. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-11 |
| [Governed tags](https://learn.microsoft.com/en-gb/azure/databricks/admin/governed-tags/) | This page provides an overview of governed tags, previously called tag policies in Beta, in Azure Databricks. To create and manage governed tags, see Create and manage governed tags. To apply tags, see Apply tags to Unity Catalog securable objects. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-11 |
| [Governed Tags](https://docs.databricks.com/en/admin/tag-policies) | Tag policies are used to control which users can assign certain tags and which values these users can select from when applying the governed tags. Governed tags are applicable to Unity Catalog objects and can be referenced in attribute based access control policies | PUBLIC_PREVIEW | - | Low | Account | 2026-02-11 |
| [Manage privileges](https://learn.microsoft.com/en-gb/azure/databricks/data-governance/unity-catalog/manage-privileges/) | This page explains how to control access to data and other objects in Unity Catalog. To learn about how this model differs from access control in the Hive metastore, see Work with the legacy Hive metastore alongside Unity Catalog. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-11 |
| [Manage privileges > Access requests and destination configuration](https://learn.microsoft.com/en-gb/azure/databricks/data-governance/unity-catalog/manage-privileges/) | Users can request access to Unity Catalog objects via Catalog Explorer, search, or direct links. Destinations include email, Slack, Teams, webhooks, or redirect URL. Sub-feature of Manage privileges. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-09 |
| [View data lineage](https://learn.microsoft.com/en-gb/azure/databricks/data-governance/unity-catalog/external-lineage) | This page describes how to update data lineage to include external assets and workflows that are run outside of Azure Databricks. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-11 |
| Lakehouse Federation Sharing | The Delta Sharing on Lakehouse Federation feature allows data providers to securely share data from its original location, bypassing the need to copy data into Databricks. By supporting provider-side materialization, the data is queried and materialized on the provider’s side before it is returned to the recipients, enabling seamless data sharing without complex network setup or credential transfers. | BETA | - | Medium — provider-side materialization runs on your infra | Account | 2026-02-05 |
| [Access control in Unity Catalog](https://learn.microsoft.com/en-gb/azure/databricks/data-governance/unity-catalog/access-control) | This page has an overview of access control in Unity Catalog, including privileges, policies, and data-level controls. | GA | - | Low | Docs | 2026-02-11 |
| [Overview](https://learn.microsoft.com/en-gb/azure/databricks/data-governance/) | Data governance is a framework of policies, processes, roles, and technical controls that ensures your organization's data is secure, trustworthy, and used responsibly throughout its lifecycle. Effective data governance enables you to maintain data quality, protect sensitive information, meet regulatory requirements, and maximize the value of your data assets. | GA | - | Low | Docs | 2026-02-11 |

## Data guides

| Feature | Description | Status (Azure) | Risk Assessment (Human) | Risk Assessment (Claude Opus 4.6) | Source | Last Checked |
|---------|-------------|--------|------------------------|-----------------------------------|--------|--------------|
| [Access data using external systems](https://learn.microsoft.com/en-gb/azure/databricks/external-access/admin) | Azure Databricks provides access to Unity Catalog tables using the Unity REST API and Apache Iceberg REST catalog. | PUBLIC_PREVIEW | - | Medium — opens UC tables to external Iceberg/REST clients | Docs | 2026-02-11 |
| [Data quality monitoring with anomaly detection (workspace level)](https://docs.databricks.com/aws/en/data-quality-monitoring/anomaly-detection) | This feature allows you to be alerted on data quality anomalies (e.g. freshness, completeness) for your tables. By learning the behaviors of each table and intelligently setting thresholds, the feature allows you to easily alert on data quality incidents across all your important tables. Currently, the feature is limited to freshness monitoring and is a library you call in a notebook, but will soon also include completeness (row count) monitoring. The table health information will also be published in Unity Catalog so data consumers know if a table has an ongoing data quality incident. | PUBLIC_PREVIEW | - | Low | Workspace | 2026-02-05 |
| [EventBridge support for file events](https://docs.databricks.com/aws/en/connect/unity-catalog/cloud-storage/manage-external-locations#using-a-provided-queue) | This preview adds support for EventBridge messages in file events, in addition to setups that use SNS routing or send messages directly to SQS. EventBridge may provide additional routing options for advanced use cases. | PUBLIC_PREVIEW | - | - | Account | 2026-02-11 |
| [Query data](https://learn.microsoft.com/en-gb/azure/databricks/query/formats/xml) | This article describes how to read and write XML files. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-11 |
| [Sample Data Exploration with Assistant](https://learn.microsoft.com/en-us/azure/databricks/discover/database-objects#natural-language) | Generate queries in the sample data page using natural language. Describe what you want to see, and the AI creates SQL queries you can run directly or open in the editor. | PUBLIC_PREVIEW | - | Low | Workspace | 2026-02-05 |
| [Custom JDBC on UC Compute](https://docs.databricks.com/aws/en/connect/jdbc-connection) | This feature enables users to connect to data sources using a custom JDBC driver through the Spark Data Source API. The new UC Connection of type JDBC, runs a user-provided JDBC driver powered by Lakeguard isolation on UC-supported compute: serverless, standard, and dedicated clusters with DBR 17.3 or higher. | BETA | - | Medium — custom JDBC drivers run user-provided code | Workspace | 2026-02-05 |
| [Connect to data sources and services](https://learn.microsoft.com/en-gb/azure/databricks/connect/managed-ingestion) | Learn how to create connections in Catalog Explorer that store authentication details for Lakeflow Connect managed ingestion sources. Any user with the USE CONNECTION privilege on the connection can then create managed ingestion pipelines from sources like Salesforce and SQL Server. | GA | - | Low | Docs | 2026-02-11 |
| [Database objects](https://learn.microsoft.com/en-gb/azure/databricks/database-objects/tags) | This page shows how to apply tags to Unity Catalog securable objects. | GA | - | Low | Docs | 2026-02-11 |

## Data sharing

| Feature | Description | Status (Azure) | Risk Assessment (Human) | Risk Assessment (Claude Opus 4.6) | Source | Last Checked |
|---------|-------------|--------|------------------------|-----------------------------------|--------|--------------|
| [Databricks Marketplace](https://learn.microsoft.com/en-gb/azure/databricks/marketplace/get-started-consumer) | This article describes how to access data products in Databricks Marketplace if you have an Azure Databricks workspace that is enabled for Unity Catalog. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-11 |
| [Delta sharing](https://learn.microsoft.com/en-gb/azure/databricks/delta-sharing/read-data-databricks) | This page describes how to read data shared with you using the Databricks-to-Databricks Delta Sharing protocol, where Databricks manages a secure connection for data sharing. Unlike the Delta Sharing open sharing protocol, the Databricks-to-Databricks protocol does not require a credential file (token-based security). | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-11 |
| [Delta Sharing with deletion vectors/column mapping](https://learn.microsoft.com/en-gb/azure/databricks/delta-sharing/) | Share tables that use deletion vectors, column mapping, or UniForm via Delta Sharing. Sub-feature of Delta Sharing. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-09 |
| [Sharing To Iceberg Clients](https://docs.databricks.com/aws/en/delta-sharing/create-share#iceberg-clients) | Enable customers to sharing Table/View/Materialized View/Streaming Table and Foreign Iceberg Table to Iceberg Clients, i.e., Spark Iceberg or Snowflake. | PUBLIC_PREVIEW | - | Medium — data shared to external Iceberg clients leaves platform | Account | 2026-02-05 |
| [Delta Sharing for Default Storage – Expanded Access](https://docs.databricks.com/aws/en/delta-sharing/create-share) | This preview extends Delta Sharing of Default Storage assets to support cross-region, cross-cloud, and open recipients. It also adds the ability for recipients to access Default Storage assets from classic clusters, giving you more flexibility in how and where you share data. | BETA | - | High — cross-region/cross-cloud sharing, open recipients | Account | 2026-02-05 |
| [Databricks-to-Databricks sharing](https://learn.microsoft.com/en-gb/azure/databricks/delta-sharing/read-data-databricks) | This page describes how to read data shared with you using the Databricks-to-Databricks Delta Sharing protocol, where Databricks manages a secure connection for data sharing. | GA | - | Low | Docs | 2026-02-08 |
| [Delta sharing](https://learn.microsoft.com/en-gb/azure/databricks/delta-sharing/) | This page introduces Delta Sharing in Azure Databricks, the secure data sharing platform that lets you share data and AI assets in Azure Databricks with users outside your organization, regardless of whether they use Azure Databricks. Delta Sharing is also the basis for Databricks Marketplace, an open forum for exchanging data products, and Clean Rooms, a secure and privacy-protecting environment where multiple parties can work together on sensitive enterprise data. | GA | - | Low | Docs | 2026-02-11 |
| [Delta sharing](https://learn.microsoft.com/en-gb/azure/databricks/delta-sharing/read-data-open) | This page describes how to read data shared with you using the Delta Sharing open sharing protocol with bearer tokens. It includes instructions for reading shared data using the following tools: | GA | - | Low | Docs | 2026-02-11 |
| [Delta Sharing open protocol](https://learn.microsoft.com/en-gb/azure/databricks/delta-sharing/read-data-open) | This page describes how to read data shared with you using the Delta Sharing open sharing protocol with bearer tokens. It includes instructions for reading shared data using the following tools: | GA | - | Medium — bearer token auth, data shared to non-Databricks recipients | Docs | 2026-02-08 |
| [Delta Sharing overview](https://learn.microsoft.com/en-gb/azure/databricks/delta-sharing/) | This page introduces Delta Sharing in Azure Databricks, the secure data sharing platform that lets you share data and AI assets in Azure Databricks with users outside your organization, regardless of whether they use Azure Databricks. | GA | - | Low | Docs | 2026-02-08 |

## Data warehousing

| Feature | Description | Status (Azure) | Risk Assessment (Human) | Risk Assessment (Claude Opus 4.6) | Source | Last Checked |
|---------|-------------|--------|------------------------|-----------------------------------|--------|--------------|
| [Queries](https://learn.microsoft.com/en-gb/azure/databricks/sql/user/queries/performance-insights) | This feature is in Private Preview. To try it, reach out to your Azure Databricks contact. | PRIVATE_PREVIEW | - | Low | Docs | 2026-02-08 |
| [SQL Alerts V2](https://docs.databricks.com/aws/en/sql/user/alerts) | The next generation of Databricks SQL Alerts - with a revamped user interface, a simplified data model, evaluation history, and improved API support. | PUBLIC_PREVIEW | - | Low | Workspace | 2026-02-05 |
| [SQL editor](https://learn.microsoft.com/en-gb/azure/databricks/sql/user/sql-editor/custom-format) | This article explains how to customize SQL auto-formatting options in the Azure Databricks UI. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-11 |
| [Metric view window measures](https://learn.microsoft.com/en-gb/azure/databricks/metric-views/data-modeling/) | Window measures for metric views enabling time-based and partition-based aggregate calculations. Sub-feature of metric views. | EXPERIMENTAL | - | Low | Docs | 2026-02-09 |
| [Allow users to enable the new SQL editor for queries](https://docs.databricks.com/en/sql/user/sql-editor/new.html) | The new SQL editor offers improved performance and introduces features like multiple statement results, Git integration, and real-time collaboration with user presence and comments. | GA | - | Low | Workspace | 2026-02-05 |
| [Default warehouse setting](https://learn.microsoft.com/azure/databricks/compute/sql-warehouse/create#set-a-user-level-default-warehouse) | The default warehouse feature allows Databricks workspace admins to define a default SQL warehouse that is pre-selected for users across SQL authoring surfaces, including Catalog Explorer, SQL Editor, Dashboards, Alerts, and Genie. Users can optionally override this default to set a customer default warehouse for themselves. | GA | - | Low | Workspace | 2026-02-05 |
| [Metric views](https://learn.microsoft.com/en-gb/azure/databricks/metric-views/data-modeling/) | This page describes how to model metric views and best practices for working with them. | GA | - | Low | Docs | 2026-02-11 |

## Developers

| Feature | Description | Status (Azure) | Risk Assessment (Human) | Risk Assessment (Claude Opus 4.6) | Source | Last Checked |
|---------|-------------|--------|------------------------|-----------------------------------|--------|--------------|
| [CI/CD](https://learn.microsoft.com/en-gb/azure/databricks/dev-tools/ci-cd/github) | GitHub Actions trigger runs of your CI/CD flows from your GitHub repositories and allow you to automate your build, test, and deployment CI/CD pipeline. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-08 |
| [Databricks Utilities](https://learn.microsoft.com/en-gb/azure/databricks/dev-tools/databricks-utils) | This article contains reference for Databricks Utilities (dbutils). The utilities provide commands that enable you to work with your Databricks environment from notebooks. For example, you can manage files and object storage, and work with secrets. dbutils are available in Python, R, and Scala notebooks. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-11 |
| [Enable networking for UDFs in Serverless SQL Warehouses](https://docs.databricks.com/aws/en/security/network/serverless-network-security/manage-network-policies#validate-with-databricks-sql) | Enables internet access from User Defined Functions (UDFs) for Serverless SQL Warehouses. UDFs can now make network calls including downloading files and sending requests. Note that this opens up exfiltration of input data to the UDFs to the external network. Enabling this preview on Serverless Warehouses takes up to 15 minutes. | PUBLIC_PREVIEW | - | High — opens internet egress from UDFs, data exfiltration risk | Workspace | 2026-02-05 |
| [Enhanced Python UDFs in Unity Catalog](https://docs.databricks.com/en/udf) | Added support for custom dependencies from PyPI in UC Python UDFs (Databricks Runtime 16.2 and above), UC service credentials and batched execution (both Databricks Runtime 16.3 and above) to Python UDFs in Unity Catalog. Available on SQL warehouses (Pro and Serverless), Serverless Jobs and Notebooks and UC-enabled clusters. To leverage UC Python UDF environments on Serverless SQL, "Enable networking for UDFs in Serverless SQL Warehouses" must be enabled as well. Enabling this preview on Serverless Warehouses, jobs and notebooks takes up to 24 hours. This preview also enables Databricks Connect Python UDF dependencies (Databricks Runtime 16.4 and above), see the docs here: https://docs.databricks.com/aws/en/dev-tools/databricks-connect/python/udf | PUBLIC_PREVIEW | - | Medium — custom PyPI deps in UDFs, supply chain risk | Workspace | 2026-02-05 |
| [Local development tools](https://learn.microsoft.com/en-gb/azure/databricks/dev-tools/cli/bundle-commands) | This information applies to Databricks CLI versions 0.205 and above. The Databricks CLI is in Public Preview. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-11 |
| [Serverless Private Git](https://learn.microsoft.com/en-us/azure/databricks/repos/serverless-private-git) | Simplify the process of using Private Git Networking with the use of Serverless Private Link.  Avaialable on AWS and Azure. | PUBLIC_PREVIEW | - | Low | Workspace | 2026-02-05 |
| [Git CLI support for Git folders](https://learn.microsoft.com/en-us/azure/databricks/repos/git-operations-with-repos#use-git-cli-commands-beta) | You can now run Git CLI commands against Git folders in the Web Terminal and from notebooks. Previously, there was only support for Git operations via the UI. | BETA | - | Low | Workspace | 2026-02-05 |
| [Local development tools](https://learn.microsoft.com/en-gb/azure/databricks/dev-tools/sdk-go) | In this article, you learn how to automate Azure Databricks operations and accelerate development with the Databricks SDK for Go. This article supplements the Databricks SDK for Go README, API reference, and examples. | BETA | - | Low | Docs | 2026-02-11 |
| [Serverless Scala and Java Jobs](https://docs.databricks.com/aws/en/jobs/jar) | Deploy Scala and Java jobs as Jars on serverless compute with faster startup, auto-scaling, and no cluster management | BETA | - | Low | Workspace | 2026-02-05 |
| [CI/CD](https://learn.microsoft.com/en-gb/azure/databricks/dev-tools/bundles/direct) | Databricks Asset Bundles was originally built on top of the Databricks Terraform provider to manage deployments. However, in an effort to move away from this dependency, Databricks CLI version 0.279.0 and above supports two different deployment engines: terraform and direct. The direct deployment engine does not depend on Terraform and will soon become the default. The Terraform deployment engine will eventually be deprecated. | EXPERIMENTAL | - | Low | Docs | 2026-02-11 |
| [dbutils.data utility](https://learn.microsoft.com/en-gb/azure/databricks/dev-tools/databricks-utils) | The Databricks Utilities data module (dbutils.data) for data exploration and summarization. | EXPERIMENTAL | - | Low | Docs | 2026-02-09 |
| [dbutils.notebook utility](https://learn.microsoft.com/en-gb/azure/databricks/dev-tools/databricks-utils) | The Databricks Utilities notebook module (dbutils.notebook) for chaining notebook executions. | EXPERIMENTAL | - | Low | Docs | 2026-02-09 |
| [Local development tools](https://learn.microsoft.com/en-gb/azure/databricks/dev-tools/databricks-connect-legacy) | Databricks Connect recommends that you use Databricks Connect for Databricks Runtime 13.0 and above instead. | GA | - | - | Docs | 2026-02-11 |
| [Local development tools](https://learn.microsoft.com/en-gb/azure/databricks/dev-tools/go-sql-driver) | The Databricks SQL Driver for Go is a Go library that allows you to use Go code to run SQL commands on Azure Databricks compute resources. This article supplements the Databricks SQL Driver for Go README, API reference, and examples. | GA | - | Low | Docs | 2026-02-11 |

## Notebooks

| Feature | Description | Status (Azure) | Risk Assessment (Human) | Risk Assessment (Claude Opus 4.6) | Source | Last Checked |
|---------|-------------|--------|------------------------|-----------------------------------|--------|--------------|
| [Databricks Assistant Agent Mode](https://learn.microsoft.com/en-us/azure/databricks/notebooks/ds-agent) | Databricks Assistant Agent Mode can automate multiple steps. From a single prompt, it can retrieve relevant assets, generate and run code, fix errors automatically, and visualize results. It can sample data and cell outputs to provide better results. | GA | - | Medium — AI agent executes code autonomously | Workspace | 2026-02-11 |

## OLTP databases (Lakebase)

| Feature | Description | Status (Azure) | Risk Assessment (Human) | Risk Assessment (Claude Opus 4.6) | Source | Last Checked |
|---------|-------------|--------|------------------------|-----------------------------------|--------|--------------|
| [Postgres -> Delta sync](https://learn.microsoft.com/en-gb/azure/databricks/oltp/) | Lakebase Postgres is a fully managed, cloud-native PostgreSQL database that brings online transaction processing (OLTP) capabilities to the Lakehouse. | PRIVATE_PREVIEW | - | - | Docs | 2026-02-11 |
| [PostgREST API support](https://learn.microsoft.com/en-gb/azure/databricks/oltp/) | Lakebase Postgres is a fully managed, cloud-native PostgreSQL database that brings online transaction processing (OLTP) capabilities to the Lakehouse. | PRIVATE_PREVIEW | - | Medium — new REST API surface for database access | Docs | 2026-02-11 |
| [Lakebase Postgres](https://learn.microsoft.com/en-us/azure/databricks/oltp) | A new Postgres compute type built for low-latency reads and writes. Autoscaling offers autoscaling compute, branching, instance restore, and more, while Provisioned provides high availability (HA) and integration with Databricks Apps. | PUBLIC_PREVIEW | - | Medium — new compute type, operational maturity risk | Workspace | 2026-02-05 |
| [Lakebase Provisioned](https://learn.microsoft.com/en-gb/azure/databricks/oltp/) | Lakebase Postgres is a fully managed, cloud-native PostgreSQL database that brings online transaction processing (OLTP) capabilities to the Lakehouse. | PUBLIC_PREVIEW | - | - | Docs | 2026-02-11 |
| [Infrastructure as code (Asset Bundles, Terraform)](https://learn.microsoft.com/en-gb/azure/databricks/oltp/) | Lakebase Postgres is a fully managed, cloud-native PostgreSQL database that brings online transaction processing (OLTP) capabilities to the Lakehouse. | BETA | - | - | Docs | 2026-02-11 |
| [Lakebase Autoscaling](https://learn.microsoft.com/en-gb/azure/databricks/oltp/) | Lakebase Postgres is a fully managed, cloud-native PostgreSQL database that brings online transaction processing (OLTP) capabilities to the Lakehouse. | BETA | - | Low | Docs | 2026-02-11 |
| [Programmatic access (REST API, CLI, SDKs)](https://learn.microsoft.com/en-gb/azure/databricks/oltp/) | Lakebase Postgres is a fully managed, cloud-native PostgreSQL database that brings online transaction processing (OLTP) capabilities to the Lakehouse. | BETA | - | - | Docs | 2026-02-11 |

## Reference

| Feature | Description | Status (Azure) | Risk Assessment (Human) | Risk Assessment (Claude Opus 4.6) | Source | Last Checked |
|---------|-------------|--------|------------------------|-----------------------------------|--------|--------------|
| [AI ParseDocument](https://docs.databricks.com/aws/en/sql/language-manual/functions/ai_parse_document) | The ai_parse_document() function invokes a state-of-the-art generative AI model from Databricks Foundation Model APIs to extract structured content from unstructured documents. | PUBLIC_PREVIEW | - | Low | Workspace | 2026-02-05 |
| [Error messages](https://learn.microsoft.com/en-gb/azure/databricks/error-messages/) | Applies to:  Databricks SQL  Databricks Runtime 12.2 and above | PUBLIC_PREVIEW | - | - | Docs | 2026-02-11 |
| [PySpark reference](https://learn.microsoft.com/en-gb/azure/databricks/pyspark/reference/functions/st_addpoint) | Adds a new point to the n-th position in the input linestring Geography or Geometry. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-11 |

## Resources

| Feature | Description | Status (Azure) | Risk Assessment (Human) | Risk Assessment (Claude Opus 4.6) | Source | Last Checked |
|---------|-------------|--------|------------------------|-----------------------------------|--------|--------------|
| [Documentation archive](https://learn.microsoft.com/en-gb/azure/databricks/archive/compute/configure) | These are instructions for the legacy create cluster UI, and are included only for historical accuracy. All customers should be using the updated create cluster UI. | PUBLIC_PREVIEW | - | - | Docs | 2026-02-11 |
| [Documentation archive](https://learn.microsoft.com/en-gb/azure/databricks/archive/legacy/uniform) | This documentation has been retired and might not be updated. The products, services, or technologies mentioned in this content are no longer supported. See Read Delta tables with Iceberg clients. | PUBLIC_PREVIEW | - | - | Docs | 2026-02-11 |
| [Documentation archive](https://learn.microsoft.com/en-gb/azure/databricks/archive/legacy-model-serving/model-serving) | This documentation has been retired and might not be updated. The products, services, or technologies mentioned in this content are no longer supported. | PUBLIC_PREVIEW | - | - | Docs | 2026-02-11 |
| [Documentation archive](https://learn.microsoft.com/en-gb/azure/databricks/archive/unity-catalog/abac-public-preview-transition) | This documentation has been retired and might not be updated. The products, services, or technologies mentioned in this content are no longer supported. See Unity Catalog attribute-based access control (ABAC). | PUBLIC_PREVIEW | - | - | Docs | 2026-02-11 |

## Security & compliance

| Feature | Description | Status (Azure) | Risk Assessment (Human) | Risk Assessment (Claude Opus 4.6) | Source | Last Checked |
|---------|-------------|--------|------------------------|-----------------------------------|--------|--------------|
| [Authentication and access control](https://learn.microsoft.com/en-gb/azure/databricks/security/auth/change-default-workspace-access) | This page explains how workspace admins can change the default workspace access for new users to consumer access by using group cloning. This feature helps you streamline consumer onboarding at scale while maintaining appropriate access levels for users who need authoring privileges. | PUBLIC_PREVIEW | - | Low — restricts new users to consumer role | Docs | 2026-02-11 |
| [Compliance](https://learn.microsoft.com/en-gb/azure/databricks/security/privacy/security-profile) | This page describes the compliance security profile, its compliance controls, and supported features. To enable the compliance security profile, see Configure enhanced security and compliance settings. | PUBLIC_PREVIEW | - | Low — security-enhancing, enforces compliance controls | Docs | 2026-02-11 |
| [Context-based Ingress Control](https://learn.microsoft.com/en-us/azure/databricks/security/network/front-end/context-based-ingress) | Context-based ingress control enables access decisions based on identity, source network and destination API scopes. Warning: disabling this preview will not disable enforcement. Update your ingress policies to allow access from all sources, or delete your ingress policies, before turning off Context-based Ingress Control. | PUBLIC_PREVIEW | - | High — ingress policy changes affect all workspace access | Account | 2026-02-05 |
| [Networking](https://learn.microsoft.com/en-gb/azure/databricks/security/network/classic/update-workspaces) | This page provides step-by-step instructions for updating the virtual network (VNet) configuration of an existing an Azure Databricks workspace. This allows you to migrate a workspace from a Azure Databricks-managed VNet to your own VNet, a process known as VNet injection, or to modify the VNet configuration of an existing VNet-injected workspace. | PUBLIC_PREVIEW | - | Medium — VNet changes affect network isolation | Docs | 2026-02-11 |
| [Secret management](https://learn.microsoft.com/en-gb/azure/databricks/security/secrets/secrets-spark-conf-env-var) | This article provides details about how to reference a secret in a Spark configuration property or environment variable. Retrieved secrets are redacted from notebook output and Spark driver and executor logs. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-08 |
| [CMK-encrypted Managed Catalogs](https://docs.databricks.com/gcp/en/security/keys/customer-managed-keys) | When enabled, this flag allows you to create Catalogs encrypted with your Customer Managed Keys (CMK). Data stored at rest is encrypted. If the specified CMK is disabled or deleted from your cloud account, we will no longer be able to access your data. Only Databricks-managed data is encrypted. Any data hosted on your personal cloud is not encrypted. | BETA | - | High — key deletion causes permanent irrecoverable data loss | Account | 2026-02-05 |
| [Enforce IP access list on Compute Plane Requests](https://community.databricks.com/t5/product-platform-updates/action-required-azure-update-outbound-connectivity-for-classic/ba-p/70060) | Enable the enforcement of IP access list on certified requests from compute plane to access your workspaces. When this preview is off, Databricks allows certified requests from the compute plane to access your workspace regardless of the workspace IP access list. | Unknown | - | High — may break compute plane connectivity if IP lists are misconfigured | Workspace | 2026-02-11 |

## Sign up for Databricks

| Feature | Description | Status (Azure) | Risk Assessment (Human) | Risk Assessment (Claude Opus 4.6) | Source | Last Checked |
|---------|-------------|--------|------------------------|-----------------------------------|--------|--------------|
| [Free Edition](https://learn.microsoft.com/en-gb/azure/databricks/getting-started/ce-migration) | With the release of Databricks Free Edition, Community Edition (CE) will soon be retired. Community Edition workspace owners should use the workspace migration tool to migrate to Free Edition as soon as possible. For a feature comparison between Community Edition and Free Edition, see Feature comparison. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-11 |

## System tables

| Feature | Description | Status (Azure) | Risk Assessment (Human) | Risk Assessment (Claude Opus 4.6) | Source | Last Checked |
|---------|-------------|--------|------------------------|-----------------------------------|--------|--------------|
| [Query History System Table](https://docs.databricks.com/en/administration-guide/system-tables/index.html#enable-system-tables) | Enables the system.query.history table, which contains performance insights for all your queries. Note that you must additionally enable the schema “query” as described in the documentation. | PUBLIC_PREVIEW | - | Low | Account | 2026-02-11 |

## Tables

| Feature | Description | Status (Azure) | Risk Assessment (Human) | Risk Assessment (Claude Opus 4.6) | Source | Last Checked |
|---------|-------------|--------|------------------------|-----------------------------------|--------|--------------|
| [Table functionality](https://learn.microsoft.com/en-gb/azure/databricks/delta/vacuum) | Predictive optimization automatically runs VACUUM on Unity Catalog managed tables. Databricks recommends enabling predictive optimizations for all Unity Catalog managed tables to simplify data maintenance and reduce storage costs. See Predictive optimization for Unity Catalog managed tables. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-11 |
| [Table functionality](https://learn.microsoft.com/en-gb/azure/databricks/data-governance/unity-catalog/filters-and-masks/) | This page provides guidance for using row filters and column masks to filter sensitive data in your tables. | PUBLIC_PREVIEW | - | Low — security-enhancing, restricts data access | Docs | 2026-02-11 |
| [Table types](https://learn.microsoft.com/en-gb/azure/databricks/tables/convert-foreign-external) | This feature is in Public Preview and is only available to participating customers at this time. To participate in the preview, apply by filling out this form. This feature only supports converting foreign tables federated using HMS and Glue Federation. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-11 |
| [Iceberg V3](https://docs.databricks.com/aws/en/iceberg/iceberg-v3) | Iceberg V3 adds performance and feature capabilities to Delta UniForm and Managed Iceberg tables. These features include deletion vectors for efficient row-level deletes, row lineage for incremental processing, and the Variant data type for semi-structured data. | BETA | - | Low | Workspace | 2026-02-05 |
| [Variant Shredding for Optimized Read Performance on Semi-Structured Data](https://learn.microsoft.com/en-us/azure/databricks/delta/variant-shredding) | Variant shredding extracts commonly occurring fields in semi-structured data into separate columns during writes. This significantly improves read performance, but incurs some overhead on writes. Variant shredding is supported in DBR 17.2 and above. | BETA | - | Low | Workspace | 2026-02-05 |
| [Optimization and performance](https://learn.microsoft.com/en-gb/azure/databricks/optimizations/isolation-level) | The isolation level of a table defines the degree to which a transaction must be isolated from modifications made by concurrent operations. Write conflicts on Azure Databricks depend on the isolation level. | GA | - | Low | Docs | 2026-02-08 |
| [Table functionality](https://learn.microsoft.com/en-gb/azure/databricks/delta/clustering) | Liquid clustering is a data layout optimization technique that replaces table partitioning and ZORDER. It simplifies table management and optimizes query performance by automatically organizing data based on clustering keys. | GA | - | Low | Docs | 2026-02-08 |

## Technology partners

| Feature | Description | Status (Azure) | Risk Assessment (Human) | Risk Assessment (Claude Opus 4.6) | Source | Last Checked |
|---------|-------------|--------|------------------------|-----------------------------------|--------|--------------|
| [BI and visualization](https://learn.microsoft.com/en-gb/azure/databricks/integrations/google-sheets) | This page describes how to use the Databricks Connector for Google Sheets to connect to Azure Databricks from Google Sheets. The Databricks Connector queries Azure Databricks data from within Google Sheets, enabling further analysis. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-11 |
| [Databricks sign-on from partner solutions](https://learn.microsoft.com/en-gb/azure/databricks/integrations/configure-oauth-dbt) | This page describes how to configure Azure Databricks sign-on from dbt Core with Microsoft Entra ID. After you complete this one-time configuration as an Azure Databricks account admin, users can connect Azure Databricks to dbt Core using single sign-on (SSO). | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-11 |
| [Ingestion](https://learn.microsoft.com/en-gb/azure/databricks/partners/ingestion/qlik) | Qlik Replicate helps you pull data from multiple data sources (Oracle, Microsoft SQL Server, SAP, mainframe and more) into Delta Lake. Replicate's automated change data capture (CDC) helps you avoid the heavy lifting of manually extracting data, transferring using an API script, chopping, staging, and importing. Qlik Compose automates the CDC into Delta Lake. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-11 |
| [Microsoft](https://learn.microsoft.com/en-gb/azure/databricks/integrations/microsoft-foundry) | This page explains how to add an Azure Databricks Genie Model Context Protocol (MCP) server as a tool in Microsoft Foundry, allowing your AI agents to connect and use the insights of Genie spaces. The Genie MCP server on Azure Databricks invokes Genie as an MCP tool and works out of the box. To learn more, see Available managed servers and What is an AI/BI Genie space. | PUBLIC_PREVIEW | - | Medium — external AI agents connect to Databricks Genie via MCP | Docs | 2026-02-11 |

## Workspace UI

| Feature | Description | Status (Azure) | Risk Assessment (Human) | Risk Assessment (Claude Opus 4.6) | Source | Last Checked |
|---------|-------------|--------|------------------------|-----------------------------------|--------|--------------|
| [Focused notebook & file editor for Git folders](https://docs.databricks.com/aws/en/workspace/workspace-browser#authoring-contexts) | Similar to opening a folder in an IDE, you can now set the scope of the notebook and file editor to a specific Git folder. When the scope is set to a Git folder, the side panel displays that folder’s contents as an expandable tree, and the editor’s tab bar displays only the files, notebooks, and queries opened while the scope is set to that folder. | PUBLIC_PREVIEW | - | Low | Workspace | 2026-02-05 |
| [Introduction to workspace objects](https://learn.microsoft.com/en-gb/azure/databricks/workspace/workspace-assets) | This article provides a high-level introduction to Azure Databricks workspace objects. You can create, view, and organize workspace objects in the workspace browser across personas. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-08 |
| [Navigate the workspace](https://learn.microsoft.com/en-gb/azure/databricks/workspace/navigate-workspace) | Learn how to navigate the Lakehouse workspace UI and find the features you need. This article explains the core concepts of the Azure Databricks workspace UI, an environment for authoring and accessing all of your Azure Databricks objects. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-11 |
| [Search for workspace objects](https://learn.microsoft.com/en-gb/azure/databricks/search/) | This article describes how to search for tables, volumes, notebooks, queries, dashboards, alerts, files, folders, libraries, jobs, repos, partners, and Marketplace listings in your Azure Databricks workspace. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-08 |
| Tagging support for workspace scoped assets | Enable tags for new workspace scoped assets including lakeview dashboards and genie spaces. Users with edit level or above permissions can add and modify tags on the asset | PUBLIC_PREVIEW | - | Low | Workspace | 2026-02-05 |
| [Workspace navigation](https://learn.microsoft.com/en-gb/azure/databricks/workspace/navigate-workspace) | Learn how to navigate the Lakehouse workspace UI and find the features you need. This article explains the core concepts of the Azure Databricks workspace UI, an environment for authoring and accessing all of your Azure Databricks objects. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-11 |
| Tree view of the side panel file browser | The file browser in the editor’s side panel now supports a tree view, allowing you to expand multiple folders at once and quickly navigate between files across directories without backtracking. | BETA | - | Low | Workspace | 2026-02-05 |
| [Databricks AI assistive features](https://learn.microsoft.com/en-gb/azure/databricks/databricks-ai/partner-powered) | The partner-powered AI features setting controls whether some features are powered by models hosted by a partner provider. By default, this setting is enabled for all workspaces that do not have a compliance security profile (CSP) enabled. For CSP workspaces, this setting is disabled by default. | GA | - | Low | Docs | 2026-02-11 |
| [Databricks One](https://docs.databricks.com/aws/en/workspace/databricks-one) | Enables the Databricks One business user experience for Consumer entitled users in the Workspace. | GA | - | Low | Workspace | 2026-02-05 |

## Apache Spark

| Feature | Description | Status (Azure) | Risk Assessment (Human) | Risk Assessment (Claude Opus 4.6) | Source | Last Checked |
|---------|-------------|--------|------------------------|-----------------------------------|--------|--------------|
| [Structured Streaming](https://learn.microsoft.com/en-gb/azure/databricks/structured-streaming/real-time) | This page describes real-time mode, a trigger type for Structured Streaming that enables ultra-low latency data processing with end-to-end latency as low as 5 ms. This mode is designed for operational workloads that require immediate response to streaming data. | PUBLIC_PREVIEW | - | Low | Docs | 2026-02-11 |

## Architecture

| Feature | Description | Status (Azure) | Risk Assessment (Human) | Risk Assessment (Claude Opus 4.6) | Source | Last Checked |
|---------|-------------|--------|------------------------|-----------------------------------|--------|--------------|
| [Lakehouse architecture](https://learn.microsoft.com/en-gb/azure/databricks/lakehouse-architecture/performance-efficiency/best-practices) | This article covers best practices for performance efficiency, organized by architectural principles listed in the following sections. | PUBLIC_PREVIEW | - | - | Docs | 2026-02-11 |

*Last updated: 2026-02-11 — 215 features tracked across 23 categories.*
